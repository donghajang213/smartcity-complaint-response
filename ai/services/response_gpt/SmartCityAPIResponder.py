from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

class SmartCityAPIResponder:
    def __init__(self, llm):
        self.llm = llm
        self.prompt = PromptTemplate.from_template(
            """
            ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ í•´ë‹¹ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ê³µê³µ API ê²°ê³¼ì…ë‹ˆë‹¤.
            ì´ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

            [ì§ˆë¬¸]
            {question}

            [ê³µê³µAPI ì •ë³´]
            {api_context}

            [ë‹µë³€]
            """
        )
        self.chain = LLMChain(llm=self.llm, prompt=self.prompt)

    def answer(self, api_results: dict):
        question = api_results["question"]
        api_context = self._format_api_results(api_results["results"])
        answer = self.chain.run(question=question, api_context=api_context)
        return {
                "answer": answer,
                "sources": api_results["results"]
            }

    def _format_api_results(self, results):
        """results: list of dicts with 'entity_results' and 'API_results'"""
        formatted_blocks = []
        for idx, r in enumerate(results):
            entity = r.get("entity_results", {})
            api_data = r.get("API_results", {})
            category = entity.get("category", f"ì¹´í…Œê³ ë¦¬{idx+1}")
            formatted_api = self._format_single_api_result(api_data)
            block = f"ğŸ“Œ [{category}]\n{formatted_api}"
            formatted_blocks.append(block)
        return "\n\n".join(formatted_blocks)

    def _format_single_api_result(self, api_data):
        if hasattr(api_data, "to_string"):
            return api_data.to_string(index=False)
        elif isinstance(api_data, dict):
            return "\n".join([f"{k}: {v}" for k, v in api_data.items()])
        elif isinstance(api_data, list):
            return "\n".join([str(item) for item in api_data])
        else:
            return str(api_data)


if __name__ == "__main__":
    from dotenv import load_dotenv
    import os
    from langchain_community.chat_models import ChatOpenAI

    load_dotenv()

    # GPT ëª¨ë¸ ì´ˆê¸°í™”
    llm = ChatOpenAI(model="gpt-4o-mini-2024-07-18", temperature=0.7)

    # ì˜ˆì‹œ API ê²°ê³¼
    api_results = {
        "question": "ì¤‘êµ¬ ë‚ ì”¨ë‘ ë¯¸ì„¸ë¨¼ì§€ ì•Œë ¤ì¤˜",
        "results": [
            {
                "entity_results": {
                    "category": "ë‚ ì”¨",
                    "intent": "ë‚ ì”¨",
                    "entities": [
                        {"type": "ì§€ì—­", "value": "ì¤‘êµ¬"},
                        {"type": "ê¸°ì˜¨", "value": "ê¸°ì˜¨"}
                    ]
                },
                "API_results": {
                    "ì§€ì—­": "ì¤‘êµ¬",
                    "ê¸°ì˜¨": "26ë„"
                }
            },
            {
                "entity_results": {
                    "category": "í™˜ê²½",
                    "intent": "ë¯¸ì„¸ë¨¼ì§€",
                    "entities": [
                        {"type": "ì§€ì—­", "value": "ì¤‘êµ¬"},
                        {"type": "ë¯¸ì„¸ë¨¼ì§€", "value": "ë¯¸ì„¸ë¨¼ì§€"}
                    ]
                },
                "API_results": {
                    "ì§€ì—­": "ì¤‘êµ¬",
                    "ë¯¸ì„¸ë¨¼ì§€": "32"
                }
            }
        ]
    }

    responder = SmartCityAPIResponder(llm=llm)
    answer = responder.answer(api_results)

    print("\nğŸ§  GPT ì‘ë‹µ:")
    print(answer)