from langchain_community.document_loaders import CSVLoader
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
import os

class CreateRAG:
    def __init__(self, file_path: str, source_column: str):
        self.file_path = file_path
        self.source_column = source_column

        # ê¸°ë³¸ ì„ë² ë”© ì„¤ì •
        self.embedding = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        self.raw_docs = None
        self.db = None

    def csv_loader(self, file_path: str, source_column: str, encoding: str = "utf-8"):
        loader = CSVLoader(
            file_path=file_path,
            encoding=encoding,
            source_column=source_column
        )
        self.raw_docs = loader.load()

    def process_documents(self):
        docs = []
        for doc in self.raw_docs:
            try:
                page_content = doc.page_content.strip()
                page_content = page_content.split("\nì§ˆë¬¸ë‚ ì§œ:")[0]
                if not page_content:
                    continue
                new_metadata = {
                    "ì§ˆë¬¸ë‚ ì§œ": doc.page_content.strip().split("\nì§ˆë¬¸ë‚ ì§œ:")[1]
                                .split("\në‹µë³€ë‚ ì§œ:")[0].strip(),
                    "ë‹µë³€ë‚ ì§œ": doc.page_content.strip().split("\në‹µë³€ë‚ ì§œ:")[1].strip()
                }
                docs.append(Document(page_content=page_content, metadata=new_metadata))
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return docs

    def split_documents(self, docs, chunk_size: int = 600, chunk_overlap: int = 50):
        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        return splitter.split_documents(docs)

    def create_vector_db(self, splits, collection_name: str = "qa_db", persist_directory: str = "db"):
        # **ë¡œì»¬(in-process) ëª¨ë“œ**: ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì˜ íŒŒì¼ ì‹œìŠ¤í…œì— ì €ì¥
        db = Chroma(
            embedding_function=self.embedding,
            collection_name=collection_name,
            persist_directory=persist_directory
        )
        db.add_documents(splits)
        db.persist()
        self.db = db
        return db

    def create_rag(
        self,
        encoding: str = "utf-8",
        chunk_size: int = 600,
        chunk_overlap: int = 50,
        collection_name: str = "qa_db",
        persist_directory: str = "db"
    ):
        print("â–¶â–¶ CreateRAG ìŠ¤í¬ë¦½íŠ¸ ì§„ì… (ë¡œì»¬ ëª¨ë“œ)")
        print("ğŸ”„ RAG ìƒì„± ì‹œì‘... (ë¡œì»¬ ëª¨ë“œ)")
        print("ğŸ—‚ CSV ë¡œë”© ì¤‘...")
        self.csv_loader(self.file_path, self.source_column, encoding)
        print(f"ğŸ“‘ ë¬¸ì„œ ê°œìˆ˜: {len(self.raw_docs)}")
        docs = self.process_documents()
        print(f"âœ‚ï¸ ë¶„í• ëœ ì²­í¬ ê°œìˆ˜: {len(docs)}")
        splits = self.split_documents(docs, chunk_size, chunk_overlap)
        print("ğŸ’¾ ë²¡í„° DB ìƒì„± ì¤‘...")
        self.create_vector_db(splits, collection_name, persist_directory)

        if self.db:
            print("âœ… DB ìƒì„± ì„±ê³µ (ë¡œì»¬ ëª¨ë“œ)")
            print("â–¶â–¶ CreateRAG ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ (ë¡œì»¬ ëª¨ë“œ)")
            return self.db
        else:
            print("âŒ DB ìƒì„±ì— ì‹¤íŒ¨í•˜ì˜€ìŠµë‹ˆë‹¤. (ë¡œì»¬ ëª¨ë“œ)")
            print("â–¶â–¶ CreateRAG ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ (ë¡œì»¬ ëª¨ë“œ)")
            return None

if __name__ == "__main__":
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(BASE_DIR, "..", "..", "data", "ë¯¼ì›ë°ì´í„°.csv")
    c_rag = CreateRAG(file_path, "ë‹µë³€ë‚ ì§œ")
    c_rag.create_rag()
